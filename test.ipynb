{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "from chat_manager import ChatManager\n",
    "from utils.token_counter import count_tokens\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversation(file_path):\n",
    "    print(f\"Loading conversation from file: {file_path}\")\n",
    "    spec = importlib.util.spec_from_file_location(\"sample_conversation\", file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"sample_conversation\"] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    print(f\"Loaded conversation with {len(module.conversation)} messages\")\n",
    "    return module.conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conversation from file: data/sample_conversation.py\n",
      "Loaded conversation with 38 messages\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation = load_conversation('data/sample_conversation.py')\n",
    "chat_manager = ChatManager()\n",
    "approaches = chat_manager.approaches.keys()\n",
    "results = {approach: [] for approach in approaches}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_manager.set_approach('advanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = 'advanced'\n",
    "cumulative_tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, message in enumerate(conversation):\n",
    "    user_id = f\"user_{approach}\"\n",
    "    if message['role'] == 'user':\n",
    "        context = chat_manager.get_context(user_id)\n",
    "        prompt = f\"Context: {context}\\n\\nUser: {message['content']}\\n\\nAssistant:\"\n",
    "        tokens = count_tokens(prompt)\n",
    "        cumulative_tokens += tokens\n",
    "        results[approach].append((i + 1, cumulative_tokens))\n",
    "        print(f\"Approach: {approach}, Message: {i+1}, Cumulative Tokens: {cumulative_tokens}\")\n",
    "    chat_manager.handle_new_message(user_id, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Can you tell me what AWS is all about?'}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i, message in enumerate(conversation):\n",
    "    print(message)\n",
    "    user_id = f\"user_{approach}\"\n",
    "    if message['role'] == 'user':\n",
    "        context = chat_manager.get_context(user_id)\n",
    "        print(context)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chat/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "class AdvancedContextManager:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.short_term_memory: List[Dict] = []\n",
    "        self.long_term_memory: List[Dict] = []\n",
    "        self.index = faiss.IndexFlatL2(384)  # 384 is the embedding dimension for the chosen model\n",
    "        self.max_short_term_tokens = 800\n",
    "        self.max_context_tokens = 2000\n",
    "\n",
    "    def add_message(self, user_id: str, message: Dict):\n",
    "        # Add to short-term memory\n",
    "        print(message)\n",
    "        self.short_term_memory.append(message)\n",
    "        print(\"short_term_memory\", self.short_term_memory)\n",
    "        self._trim_short_term_memory()\n",
    "\n",
    "        # Add to long-term memory\n",
    "        embedding = self._get_embedding(message['content'])\n",
    "        print(\"embedding\", embedding)\n",
    "        self.long_term_memory.append({**message, 'embedding': embedding})\n",
    "        self.index.add(np.array([embedding]))\n",
    "\n",
    "    def get_context(self, user_id: str) -> str:\n",
    "        context = self._get_short_term_context()\n",
    "        long_term_context = self._get_long_term_context(user_id)\n",
    "        return json.dumps(context + long_term_context)\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    def _trim_short_term_memory(self):\n",
    "        while len(self.short_term_memory) > 4 or self._count_tokens(self.short_term_memory) > self.max_short_term_tokens:\n",
    "            self.short_term_memory.pop(0)\n",
    "\n",
    "    def _get_short_term_context(self) -> List[Dict]:\n",
    "        return self.short_term_memory\n",
    "\n",
    "    def _get_long_term_context(self, user_id: str) -> List[Dict]:\n",
    "        if not self.long_term_memory:\n",
    "            return []\n",
    "\n",
    "        query = self.short_term_memory[-1]['content'] if self.short_term_memory else \"\"\n",
    "        query_embedding = self._get_embedding(query)\n",
    "\n",
    "        k = min(5, len(self.long_term_memory))  # Get top 5 or all if less than 5\n",
    "        D, I = self.index.search(np.array([query_embedding]), k)\n",
    "\n",
    "        results = [self.long_term_memory[i] for i in I[0]]\n",
    "        results.sort(key=lambda x: x['timestamp'], reverse=True)  # Sort by recency\n",
    "\n",
    "        return results[:2]  # Return top 2 most relevant and recent\n",
    "\n",
    "    def _count_tokens(self, messages: List[Dict]) -> int:\n",
    "        return sum(len(self.tokenizer.encode(msg['content'])) for msg in messages)\n",
    "\n",
    "    def get_internal_state(self, user_id: str) -> Dict:\n",
    "        return {\n",
    "            \"short_term_memory\": self.short_term_memory,\n",
    "            \"long_term_memory_size\": len(self.long_term_memory),\n",
    "            \"current_context_tokens\": self._count_tokens(self.short_term_memory)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add the parent directory to the Python path\n",
    "\n",
    "# from advanced_context_manager import AdvancedContextManager\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "context_manager = AdvancedContextManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chat/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='sentence-transformers/all-MiniLM-L6-v2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': 'Hello, how are you?',\n",
       " 'timestamp': '2024-10-11T01:12:48.438125'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embedding = context_manager._get_embedding(messages[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(role, content, timestamp=None):\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "    return {\"role\": role, \"content\": content, \"timestamp\": timestamp}\n",
    "\n",
    "messages = [\n",
    "    create_message(\"user\", \"Hello, how are you?\"),\n",
    "    create_message(\"assistant\", \"I'm doing well, thank you for asking. How can I assist you today?\"),\n",
    "    create_message(\"user\", \"Can you tell me about the weather today?\"),\n",
    "    create_message(\"assistant\", \"I'm sorry, but I don't have access to real-time weather information. Is there something else I can help you with?\"),\n",
    "    create_message(\"user\", \"Okay, can you explain the concept of machine learning?\"),\n",
    "    create_message(\"assistant\", \"Certainly! Machine learning is a subset of artificial intelligence...\"),\n",
    "    # Add more messages as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, message in enumerate(messages):\n",
    "    context_manager.add_message(\"test_user\", message)\n",
    "    print(f\"After message {i+1}:\")\n",
    "    print(json.loads(context_manager.get_context(\"test_user\")))\n",
    "    print(\"\\nInternal state:\")\n",
    "    print(context_manager.get_internal_state(\"test_user\"))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
